{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f7cdf60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T17:37:21.316604Z",
     "iopub.status.busy": "2025-01-12T17:37:21.316193Z",
     "iopub.status.idle": "2025-01-12T17:37:27.504860Z",
     "shell.execute_reply": "2025-01-12T17:37:27.504114Z"
    },
    "id": "vdMi4cTEsHSM",
    "papermill": {
     "duration": 6.194631,
     "end_time": "2025-01-12T17:37:27.506537",
     "exception": false,
     "start_time": "2025-01-12T17:37:21.311906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import models\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "730fa8c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T17:37:27.512284Z",
     "iopub.status.busy": "2025-01-12T17:37:27.511863Z",
     "iopub.status.idle": "2025-01-12T17:37:36.536790Z",
     "shell.execute_reply": "2025-01-12T17:37:36.535628Z"
    },
    "papermill": {
     "duration": 9.029118,
     "end_time": "2025-01-12T17:37:36.538279",
     "exception": false,
     "start_time": "2025-01-12T17:37:27.509161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-pretrained-biggan\r\n",
      "  Downloading pytorch_pretrained_biggan-0.1.1-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-biggan) (2.4.1+cu121)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-biggan) (1.26.4)\r\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-biggan) (1.35.83)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-biggan) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-biggan) (4.66.5)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-biggan) (3.16.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-biggan) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-biggan) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-biggan) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-biggan) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-biggan) (2024.6.1)\r\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.83 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch-pretrained-biggan) (1.35.83)\r\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch-pretrained-biggan) (1.0.1)\r\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch-pretrained-biggan) (0.10.4)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-biggan) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-biggan) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-biggan) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-biggan) (2024.8.30)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.83->boto3->pytorch-pretrained-biggan) (2.8.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->pytorch-pretrained-biggan) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->pytorch-pretrained-biggan) (1.3.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.83->boto3->pytorch-pretrained-biggan) (1.16.0)\r\n",
      "Downloading pytorch_pretrained_biggan-0.1.1-py3-none-any.whl (27 kB)\r\n",
      "Installing collected packages: pytorch-pretrained-biggan\r\n",
      "Successfully installed pytorch-pretrained-biggan-0.1.1\r\n",
      "Collecting ranger-adabelief\r\n",
      "  Downloading ranger_adabelief-0.1.0-py3-none-any.whl.metadata (549 bytes)\r\n",
      "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from ranger-adabelief) (2.4.1+cu121)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger-adabelief) (3.16.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger-adabelief) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger-adabelief) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger-adabelief) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger-adabelief) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger-adabelief) (2024.6.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->ranger-adabelief) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.0->ranger-adabelief) (1.3.0)\r\n",
      "Downloading ranger_adabelief-0.1.0-py3-none-any.whl (5.6 kB)\r\n",
      "Installing collected packages: ranger-adabelief\r\n",
      "Successfully installed ranger-adabelief-0.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-pretrained-biggan\n",
    "!pip install ranger-adabelief\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6e07a0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T17:37:36.545135Z",
     "iopub.status.busy": "2025-01-12T17:37:36.544871Z",
     "iopub.status.idle": "2025-01-12T17:37:40.507984Z",
     "shell.execute_reply": "2025-01-12T17:37:40.506833Z"
    },
    "papermill": {
     "duration": 3.968254,
     "end_time": "2025-01-12T17:37:40.509527",
     "exception": false,
     "start_time": "2025-01-12T17:37:36.541273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNet loaded successfully. Example synsets: [Synset('example.n.01'), Synset('model.n.07'), Synset('exemplar.n.01'), Synset('example.n.04'), Synset('case.n.01'), Synset('exercise.n.04')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import nltk\n",
    "\n",
    "# Define the expected NLTK directory structure\n",
    "nltk_data_path = '/kaggle/working/nltk_data/corpora/'\n",
    "os.makedirs(nltk_data_path, exist_ok=True)\n",
    "\n",
    "# Path to the uploaded wordnet directory\n",
    "wordnet_source_path = '/kaggle/input/wordnetn/wordnet/'\n",
    "wordnet_dest_path = os.path.join(nltk_data_path, 'wordnet')\n",
    "\n",
    "# Move or copy the wordnet folder to the expected location\n",
    "if not os.path.exists(wordnet_dest_path):\n",
    "    shutil.copytree(wordnet_source_path, wordnet_dest_path)\n",
    "\n",
    "# Add the working directory to NLTK's data path\n",
    "nltk.data.path.append('/kaggle/working/nltk_data/')\n",
    "# Test WordNet\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "try:\n",
    "    syns = wordnet.synsets('example')\n",
    "    print(f\"WordNet loaded successfully. Example synsets: {syns}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading WordNet: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7e8b253",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T17:37:40.516745Z",
     "iopub.status.busy": "2025-01-12T17:37:40.516300Z",
     "iopub.status.idle": "2025-01-12T17:37:40.522374Z",
     "shell.execute_reply": "2025-01-12T17:37:40.521561Z"
    },
    "papermill": {
     "duration": 0.011038,
     "end_time": "2025-01-12T17:37:40.523760",
     "exception": false,
     "start_time": "2025-01-12T17:37:40.512722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('horse.n.01'), Synset('horse.n.02'), Synset('cavalry.n.01'), Synset('sawhorse.n.01'), Synset('knight.n.02'), Synset('horse.v.01')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.data.path.append('/kaggle/working/nltk_data')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "print(wordnet.synsets('horse'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7a58eb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T17:37:40.530543Z",
     "iopub.status.busy": "2025-01-12T17:37:40.530287Z",
     "iopub.status.idle": "2025-01-12T17:58:29.182963Z",
     "shell.execute_reply": "2025-01-12T17:58:29.181839Z"
    },
    "papermill": {
     "duration": 1248.664007,
     "end_time": "2025-01-12T17:58:29.190686",
     "exception": false,
     "start_time": "2025-01-12T17:37:40.526679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210849368/210849368 [00:02<00:00, 72495884.46B/s]\n",
      "100%|██████████| 630/630 [00:00<00:00, 2297749.15B/s]\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_pretrained_biggan/model.py:279: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(resolved_model_file, map_location='cpu' if not torch.cuda.is_available() else None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images...\n",
      "Class antelope: 250 images added.\n",
      "No synset found for grizzly+bear. Skipping.\n",
      "Class grizzly+bear: 0 images added.\n",
      "No synset found for killer+whale. Skipping.\n",
      "Class killer+whale: 0 images added.\n",
      "Class beaver: 250 images added.\n",
      "Class dalmatian: 250 images added.\n",
      "No synset found for persian+cat. Skipping.\n",
      "Class persian+cat: 0 images added.\n",
      "Class horse: 500 images added.\n",
      "No synset found for german+shepherd. Skipping.\n",
      "Class german+shepherd: 0 images added.\n",
      "No synset found for blue+whale. Skipping.\n",
      "Class blue+whale: 0 images added.\n",
      "No synset found for siamese+cat. Skipping.\n",
      "Class siamese+cat: 0 images added.\n",
      "Error generating images for skunk: \n",
      "Class skunk: 0 images added.\n",
      "Error generating images for mole: \n",
      "Class mole: 0 images added.\n",
      "Class tiger: 250 images added.\n",
      "Class hippopotamus: 250 images added.\n",
      "Class leopard: 250 images added.\n",
      "Error generating images for moose: \n",
      "Class moose: 0 images added.\n",
      "No synset found for spider+monkey. Skipping.\n",
      "Class spider+monkey: 0 images added.\n",
      "No synset found for humpback+whale. Skipping.\n",
      "Class humpback+whale: 0 images added.\n",
      "Class elephant: 250 images added.\n",
      "Class gorilla: 500 images added.\n",
      "Class ox: 250 images added.\n",
      "Class fox: 500 images added.\n",
      "Class sheep: 500 images added.\n",
      "Error generating images for seal: \n",
      "Class seal: 0 images added.\n",
      "Class chimpanzee: 500 images added.\n",
      "Class hamster: 250 images added.\n",
      "Error generating images for squirrel: \n",
      "Class squirrel: 0 images added.\n",
      "Error generating images for rhinoceros: \n",
      "Class rhinoceros: 0 images added.\n",
      "Class rabbit: 500 images added.\n",
      "Class bat: 250 images added.\n",
      "Error generating images for giraffe: \n",
      "Class giraffe: 0 images added.\n",
      "Class wolf: 250 images added.\n",
      "Class chihuahua: 250 images added.\n",
      "Error generating images for rat: \n",
      "Class rat: 0 images added.\n",
      "Class weasel: 250 images added.\n",
      "Class otter: 250 images added.\n",
      "Class buffalo: 250 images added.\n",
      "Class zebra: 250 images added.\n",
      "No synset found for giant+panda. Skipping.\n",
      "Class giant+panda: 0 images added.\n",
      "Error generating images for deer: \n",
      "Class deer: 0 images added.\n",
      "Class bobcat: 250 images added.\n",
      "Class pig: 250 images added.\n",
      "Class lion: 250 images added.\n",
      "Class mouse: 250 images added.\n",
      "No synset found for polar+bear. Skipping.\n",
      "Class polar+bear: 0 images added.\n",
      "Class collie: 500 images added.\n",
      "Error generating images for walrus: \n",
      "Class walrus: 0 images added.\n",
      "Error generating images for raccoon: \n",
      "Class raccoon: 0 images added.\n",
      "Error generating images for cow: \n",
      "Class cow: 0 images added.\n",
      "Error generating images for dolphin: \n",
      "Class dolphin: 0 images added.\n",
      "Total new images added: 8500\n",
      "Final dataset size: 18044\n",
      "Training set size: 14435\n",
      "Validation set size: 3609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc60092a981340ca8f09691350becb1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/77.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Batch [100/452], Train Loss: 0.5644, Train Acc: 62.22%\n",
      "Epoch [1/5], Batch [200/452], Train Loss: 0.4057, Train Acc: 73.41%\n",
      "Epoch [1/5], Batch [300/452], Train Loss: 0.4757, Train Acc: 77.85%\n",
      "Epoch [1/5], Batch [400/452], Train Loss: 0.4891, Train Acc: 80.16%\n",
      "Epoch [1/5] Complete, Train Loss: 0.6971, Train Acc: 80.97%, Val Loss: 0.3030, Val Acc: 90.39%\n",
      "Epoch [2/5], Batch [100/452], Train Loss: 0.5580, Train Acc: 91.91%\n",
      "Epoch [2/5], Batch [200/452], Train Loss: 0.1942, Train Acc: 91.67%\n",
      "Epoch [2/5], Batch [300/452], Train Loss: 0.3113, Train Acc: 91.77%\n",
      "Epoch [2/5], Batch [400/452], Train Loss: 0.2390, Train Acc: 91.74%\n",
      "Epoch [2/5] Complete, Train Loss: 0.2545, Train Acc: 91.68%, Val Loss: 0.3317, Val Acc: 89.53%\n",
      "Epoch [3/5], Batch [100/452], Train Loss: 0.1658, Train Acc: 94.88%\n",
      "Epoch [3/5], Batch [200/452], Train Loss: 0.1219, Train Acc: 95.12%\n",
      "Epoch [3/5], Batch [300/452], Train Loss: 0.2835, Train Acc: 95.23%\n",
      "Epoch [3/5], Batch [400/452], Train Loss: 0.0292, Train Acc: 95.33%\n",
      "Epoch [3/5] Complete, Train Loss: 0.1464, Train Acc: 95.27%, Val Loss: 0.2683, Val Acc: 92.05%\n",
      "Epoch [4/5], Batch [100/452], Train Loss: 0.1477, Train Acc: 97.25%\n",
      "Epoch [4/5], Batch [200/452], Train Loss: 0.0177, Train Acc: 97.53%\n",
      "Epoch [4/5], Batch [300/452], Train Loss: 0.0777, Train Acc: 97.56%\n",
      "Epoch [4/5], Batch [400/452], Train Loss: 0.0180, Train Acc: 97.66%\n",
      "Epoch [4/5] Complete, Train Loss: 0.0717, Train Acc: 97.66%, Val Loss: 0.2416, Val Acc: 92.68%\n",
      "Epoch [5/5], Batch [100/452], Train Loss: 0.0633, Train Acc: 98.69%\n",
      "Epoch [5/5], Batch [200/452], Train Loss: 0.0034, Train Acc: 98.59%\n",
      "Epoch [5/5], Batch [300/452], Train Loss: 0.0020, Train Acc: 98.59%\n",
      "Epoch [5/5], Batch [400/452], Train Loss: 0.0663, Train Acc: 98.70%\n",
      "Epoch [5/5] Complete, Train Loss: 0.0407, Train Acc: 98.70%, Val Loss: 0.2367, Val Acc: 93.85%\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from pytorch_pretrained_biggan import BigGAN, one_hot_from_names, truncated_noise_sample\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# BigGAN model\n",
    "biggan = BigGAN.from_pretrained('biggan-deep-128').to(device)\n",
    "\n",
    "# File paths\n",
    "classes_file = \"/kaggle/input/vlg-recruitment-24-challenge/vlg-dataset/vlg-dataset/classes.txt\"\n",
    "predicate_matrix_file = \"/kaggle/input/vlg-recruitment-24-challenge/vlg-dataset/vlg-dataset/predicate-matrix-continuous.txt\"\n",
    "train_dir = \"/kaggle/input/vlg-recruitment-24-challenge/vlg-dataset/vlg-dataset/train\"\n",
    "\n",
    "# Load classes and predicate matrix\n",
    "def load_classes(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        classes = [line.strip().split('\\t')[-1] for line in f.readlines()]\n",
    "    return classes\n",
    "\n",
    "classes = load_classes(classes_file)\n",
    "folder_classes = set(os.listdir(train_dir))\n",
    "unseen_classes = [cls for cls in classes if cls not in folder_classes]\n",
    "\n",
    "# Mapping classes to indices\n",
    "class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "unseen_class_to_idx = {cls: idx + len(folder_classes) for idx, cls in enumerate(unseen_classes)}\n",
    "\n",
    "# Manual synset mapping for problematic classes\n",
    "manual_synsets = {\n",
    "    \"moose\": \"elk.n.01\",\n",
    "    \"rhinoceros\": \"rhinoceros.n.01\",\n",
    "    \"squirrel\": \"squirrel.n.01\"\n",
    "}\n",
    "\n",
    "# Dataset class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, class_to_idx, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        for class_name in os.listdir(root_dir):\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                class_idx = class_to_idx[class_name]\n",
    "                for img_name in os.listdir(class_path):\n",
    "                    img_path = os.path.join(class_path, img_name)\n",
    "                    self.data.append((img_path, class_idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):  \n",
    "        \n",
    "        img_path, label = self.data[idx]\n",
    "        # Check if the data is already a PIL Image\n",
    "        if isinstance(img_path, Image.Image):\n",
    "            image = img_path\n",
    "        else:\n",
    "            # Open the image file\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = ImageDataset(train_dir, class_to_idx, transform)\n",
    "\n",
    "def generate_images_for_class(class_name, num_images, class_idx):\n",
    "    generated_images = []\n",
    "    try:\n",
    "        synsets = wordnet.synsets(class_name.replace(\"+\", \" \"))\n",
    "        if not synsets:\n",
    "            print(f\"No synset found for {class_name}. Skipping.\")\n",
    "            return []\n",
    "        \n",
    "        synset_name = manual_synsets.get(class_name) if class_name in manual_synsets else synsets[0].name()\n",
    "\n",
    "        for batch_start in range(0, num_images, 50):\n",
    "            batch_size = min(50, num_images - batch_start)\n",
    "            class_vector = torch.tensor(\n",
    "                one_hot_from_names([synset_name.split('.')[0]], batch_size=batch_size), dtype=torch.float32\n",
    "            ).to(device)\n",
    "            noise_vector = torch.tensor(\n",
    "                truncated_noise_sample(truncation=0.4, batch_size=batch_size, seed=42), dtype=torch.float32\n",
    "            ).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = biggan(noise_vector, class_vector, truncation=0.4)\n",
    "\n",
    "            for i in range(output.shape[0]):\n",
    "                image = transforms.ToPILImage()(output[i].cpu().squeeze())\n",
    "                generated_images.append((image, class_idx))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating images for {class_name}: {e}\")\n",
    "\n",
    "    return generated_images\n",
    "\n",
    "# Generate synthetic data\n",
    "print(\"Generating images...\")\n",
    "new_data = []\n",
    "\n",
    "for class_name in classes:\n",
    "    class_idx = class_to_idx.get(class_name, unseen_class_to_idx.get(class_name))\n",
    "    if class_name in folder_classes:\n",
    "        num_images = 250\n",
    "    else:\n",
    "        num_images = 500\n",
    "\n",
    "    generated_images = generate_images_for_class(class_name, num_images, class_idx)\n",
    "    new_data.extend(generated_images)\n",
    "    print(f\"Class {class_name}: {len(generated_images)} images added.\")\n",
    "\n",
    "# Add generated data to the dataset\n",
    "for img, label in new_data:\n",
    "    dataset.data.append((img, label))\n",
    "\n",
    "# Final dataset size\n",
    "print(f\"Total new images added: {len(new_data)}\")\n",
    "print(f\"Final dataset size: {len(dataset)}\")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, valid_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(valid_ds, batch_size=32, shuffle=False, num_workers=8)\n",
    "\n",
    "print(f\"Training set size: {len(train_ds)}\")\n",
    "print(f\"Validation set size: {len(valid_ds)}\")\n",
    "\n",
    "import timm\n",
    "\n",
    "# Model initialization using Swin Transformer\n",
    "model = timm.create_model('efficientnet_b4', pretrained=True, num_classes=50)\n",
    "model = model.to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "# Define loss, optimizer, and scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4) \n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)\n",
    "\n",
    "\n",
    "# Training and validation loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Metrics calculation\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx + 1}/{len(train_loader)}], Train Loss: {loss.item():.4f}, Train Acc: {100 * correct_train / total_train:.2f}%\")\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100 * correct_train / total_train\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Metrics calculation\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = 100 * correct_val / total_val\n",
    "\n",
    "    # Scheduler step\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Complete, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64b570b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T17:58:29.205620Z",
     "iopub.status.busy": "2025-01-12T17:58:29.205333Z",
     "iopub.status.idle": "2025-01-12T18:00:38.216294Z",
     "shell.execute_reply": "2025-01-12T18:00:38.215326Z"
    },
    "papermill": {
     "duration": 129.020603,
     "end_time": "2025-01-12T18:00:38.218228",
     "exception": false,
     "start_time": "2025-01-12T17:58:29.197625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "\n",
    "# Prediction\n",
    "test_dir = \"/kaggle/input/vlg-recruitment-24-challenge/vlg-dataset/vlg-dataset/test\"\n",
    "model.eval()\n",
    "test_images = [f for f in os.listdir(test_dir) if f.endswith('.jpg')]\n",
    "test_predictions = []\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "for img_name in test_images:\n",
    "    img_path = os.path.join(test_dir, img_name)\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "        class_name = classes[predicted_class]  # Get the class name\n",
    "        test_predictions.append((img_name, class_name))\n",
    "\n",
    "# Save Predictions\n",
    "submission = pd.DataFrame(test_predictions, columns=['image_id', 'class'])\n",
    "submission.to_csv(\"/kaggle/working/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10740331,
     "sourceId": 90860,
     "sourceType": "competition"
    },
    {
     "datasetId": 6466119,
     "sourceId": 10446141,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1402.969045,
   "end_time": "2025-01-12T18:00:41.682264",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-12T17:37:18.713219",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "06afedeac42042eb96b6e7d858405b9c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "103a72a3e5514b619518e6612e5b5771": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1b2ed04b9cde4c6ba322c9a0ce84001a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "34e0c083b1074c4d8718ae01cfa7e14c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3f6938f68f9044b4a343417a9697f67b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_06afedeac42042eb96b6e7d858405b9c",
       "placeholder": "​",
       "style": "IPY_MODEL_103a72a3e5514b619518e6612e5b5771",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors: 100%"
      }
     },
     "49fd74c6446c44e09361ef28d5902998": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1b2ed04b9cde4c6ba322c9a0ce84001a",
       "placeholder": "​",
       "style": "IPY_MODEL_596a7eff800e430ab78b8332a06e1d8f",
       "tabbable": null,
       "tooltip": null,
       "value": " 77.9M/77.9M [00:00&lt;00:00, 189MB/s]"
      }
     },
     "4e920d9e71c646e882a954efc9ae1066": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "596a7eff800e430ab78b8332a06e1d8f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a79bcc214214437eb7e43f2f9fce9c4b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b995c9ce05ea44838919b1f22f43e4a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4e920d9e71c646e882a954efc9ae1066",
       "max": 77933206.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a79bcc214214437eb7e43f2f9fce9c4b",
       "tabbable": null,
       "tooltip": null,
       "value": 77933206.0
      }
     },
     "cc60092a981340ca8f09691350becb1c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3f6938f68f9044b4a343417a9697f67b",
        "IPY_MODEL_b995c9ce05ea44838919b1f22f43e4a1",
        "IPY_MODEL_49fd74c6446c44e09361ef28d5902998"
       ],
       "layout": "IPY_MODEL_34e0c083b1074c4d8718ae01cfa7e14c",
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
